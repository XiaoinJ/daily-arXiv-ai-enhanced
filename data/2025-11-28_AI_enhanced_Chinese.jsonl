{"id": "2511.21690", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21690", "abs": "https://arxiv.org/abs/2511.21690", "authors": ["Seungjae Lee", "Yoonkyo Jung", "Inkook Chun", "Yao-Chih Lee", "Zikui Cai", "Hongjia Huang", "Aayush Talreja", "Tan Dat Dao", "Yongyuan Liang", "Jia-Bin Huang", "Furong Huang"], "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos", "comment": null, "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.", "AI": {"tldr": "TraceGen\u662f\u4e00\u4e2a\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc73D\u8f68\u8ff9\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u8de8\u5e73\u53f0\u3001\u8de8\u73af\u5883\u548c\u8de8\u4efb\u52a1\u7684\u673a\u5668\u4eba\u6280\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u5c31\u80fd\u9ad8\u6548\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4ece\u5c11\u91cf\u6f14\u793a\u4e2d\u5b66\u4e60\u65b0\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5229\u7528\u4e30\u5bcc\u7684\u8de8\u5e73\u53f0\u89c6\u9891\u6570\u636e\uff08\u4eba\u7c7b\u548c\u5176\u4ed6\u673a\u5668\u4eba\uff09\uff0c\u514b\u670d\u5e73\u53f0\u5dee\u5f02\u3001\u76f8\u673a\u5dee\u5f02\u548c\u73af\u5883\u5dee\u5f02\u5e26\u6765\u7684\u969c\u788d\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u7b26\u53f7\u8868\u793a\u2014\u20143D\u8f68\u8ff9\u7a7a\u95f4\uff0c\u5f00\u53d1TraceGen\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u8f68\u8ff9\u7a7a\u95f4\u4e2d\u7684\u672a\u6765\u8fd0\u52a8\uff0c\u5e76\u6784\u5efaTraceForge\u6570\u636e\u7ba1\u9053\u5c06\u5f02\u6784\u89c6\u9891\u8f6c\u6362\u4e3a\u4e00\u81f4\u76843D\u8f68\u8ff9\u6570\u636e\u3002", "result": "\u5728123K\u89c6\u9891\u548c180\u4e07\u89c2\u5bdf-\u8f68\u8ff9-\u8bed\u8a00\u4e09\u5143\u7ec4\u4e0a\u9884\u8bad\u7ec3\uff0c\u4ec5\u97005\u4e2a\u76ee\u6807\u673a\u5668\u4eba\u89c6\u9891\u5373\u53ef\u57284\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u523080%\u6210\u529f\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5feb50-600\u500d\uff1b\u4ec5\u75285\u4e2a\u624b\u6301\u624b\u673a\u62cd\u6444\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4ecd\u80fd\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fbe\u523067.5%\u6210\u529f\u7387\u3002", "conclusion": "TraceGen\u901a\u8fc73D\u8f68\u8ff9\u7a7a\u95f4\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u6570\u636e\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u5e73\u53f0\u7684\u6280\u80fd\u8fc1\u79fb\uff0c\u65e0\u9700\u4f9d\u8d56\u7269\u4f53\u68c0\u6d4b\u5668\u6216\u7e41\u91cd\u7684\u50cf\u7d20\u7a7a\u95f4\u751f\u6210\u3002"}}
