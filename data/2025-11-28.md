<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/abs/2511.21690)
*Seungjae Lee,Yoonkyo Jung,Inkook Chun,Yao-Chih Lee,Zikui Cai,Hongjia Huang,Aayush Talreja,Tan Dat Dao,Yongyuan Liang,Jia-Bin Huang,Furong Huang*

Main category: cs.RO

TL;DR: TraceGen是一个世界模型，通过3D轨迹空间表示学习跨平台、跨环境和跨任务的机器人技能，仅需少量演示就能高效适应新任务。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从少量演示中学习新任务的挑战，利用丰富的跨平台视频数据（人类和其他机器人），克服平台差异、相机差异和环境差异带来的障碍。

Method: 提出统一的符号表示——3D轨迹空间，开发TraceGen世界模型预测轨迹空间中的未来运动，并构建TraceForge数据管道将异构视频转换为一致的3D轨迹数据。

Result: 在123K视频和180万观察-轨迹-语言三元组上预训练，仅需5个目标机器人视频即可在4个任务上达到80%成功率，推理速度比现有视频世界模型快50-600倍；仅用5个手持手机拍摄的人类演示视频仍能在真实机器人上达到67.5%成功率。

Conclusion: TraceGen通过3D轨迹空间表示有效解决了小数据学习问题，实现了跨平台的技能迁移，无需依赖物体检测器或繁重的像素空间生成。

Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

</details>
